---
title: "Practical"
---

```{r}
#| echo: false
#| message: false
library(foreach)
library(doParallel)
library(tidyverse)
library(MASS)
library(iterators)
library(doRNG)
 

#Function that takes system time results and prints an interpretable table
tablefy <- function(d, heading) {
data.frame(d)[1:3,] |> 
  t() |> 
  knitr::kable(col.names = c("User time", "System Time", "Total elapsed"), caption = heading)
}
```

# Question 1

```{r}

#Initialize cluster
cl <- makeCluster(10)
registerDoParallel(cl)





#Loop
results <- foreach(i = 1:100 ,.combine = rbind ) %dopar% {
  vals <- rexp(100,1)
  c(mean(vals), var(vals))
  
}

stopCluster(cl)

results |> 
  data.frame() |> 
  tibble::remove_rownames() |> 
  knitr::kable(col.names = c("Means", "Variances"), digits = 3)
```

# Question 2

Below are serial and parallel bootstrapping computations of 1 sample at a time each of size 1000.

```{r}
#| message: false


gal <- galaxies

#1 Bootstrap sample of 1000 at a time

#Serial 
tablefy(
  system.time(
    result1 <- foreach(i = 1:100000, .combine = c ) %do% {
      res <- sample(gal, replace = TRUE)
      median(res)
    }
  ), 
  "Serial single sample bootstrap"
)

#Parallel

cl <- makeCluster(10)
registerDoParallel(cl)

tablefy(
  system.time(
    result2 <- foreach(i = 1:100000, .combine = c ) %dopar% {
      res <- sample(gal, replace = TRUE)
      median(res)
    }
  ), 
  "Parallel single sample bootstrap"
)

stopCluster(cl)




```

Clearly the use of parallelisation did not result in a more efficient computation, there was little to not difference in the user (CPU) time but there was in increase in system (OS) time, likely due to task scheduling of the parallel tasks. Clearly in this case parallelising the computation did not result in any efficiency, it is likely that the quantity of data that had to be returned was not large enough to justify the use of parallel computing, this idea is explored below.

Below are serial and parallel bootstraps of 1000 samples at a time, each of size 1000.

```{r}
#| message: false


#1 00 bootstrap samples of 1000 at a time 


#Serial 
tablefy(
  system.time(
    result1 <- foreach(i = 1:1000, .combine = c ) %do% {
      inner <- foreach(j = 1:1000, .combine = c) %do% {
        res <- sample(gal, replace = TRUE)
        median(res)
      }
      inner
    }
  ), "Serial bootstrap of 1000 samples" 
)

#Parallel

cl <- makeCluster(10)
registerDoParallel(cl)

tablefy(system.time(
  result2 <- foreach(i = 1:1000, .combine = rbind, .packages = 'foreach' ) %dopar% {
    inner <- foreach(j = 1:1000, .combine = c, .packages = 'foreach') %dopar% {
    res <- sample(gal, replace = TRUE)
    median(res)
    }
    return(inner)
  }
), "Parallel bootstrap of 1000 samples"
)

stopCluster(cl)



```

Clearly the use of parallel computing resulting in a much faster computation and leads one ot believe that it would provide efficiency in a situation where the quantity of data that needs to be returned is large.

# Question 3

Below we estimate the coverage of a percentile bootstrap confidence interval for samples of size 50 from an exponential distribution of mean 1.

This will be implemented by generating 500 samples of size 10000 from the exponential distribution with mean 1. For each of these samples a 500 bootstrapped mean estimates will be sampled. For each of the 500 samples a 95% percentile bootstrap confidence interval of the mean will be generated. It will be thereafter be recorded as to whether 1 lies inside each of these intervals, the proportion of intervals than contain 1 will be the estimated coverage

The result is shown below.

```{r}

cl <- makeCluster(10)
registerDoParallel(cl)


#Generate exponential sample 


inCi <- foreach(i = 1:500, .combine = c, .packages = 'foreach') %dopar%{
  smpl_init <- rexp(10000, rate = 1)
  means <- foreach(i = 1:500, .combine = c, .packages = 'foreach') %dopar% {
    bsampls <- sample(smpl_init, replace = T)
    return(mean(bsampls))
  }
  
  quants <- quantile(means, probs = c(0.025, 0.975))
  
  return(((quants[1] <= 1) && (quants[2] >= 1)))
}

paste0(mean(inCi)*100, "%") |> 
  knitr::kable(col.names = "Estimated Coverage")

stopCluster(cl)
```

The estimated coverage is 95.2%, this is not a surprising figure as each bootstrap percentile confidence interval is a 95% confidence interval.

# Question 4

Below is a serial iteration over 3 vectors each containing 5 random variables and finds the max of those vectors. It is parallel safe and on a larger scale can easily be parallelised to provide more computationally efficient solutions.

```{r}


#Set seed for reproducibility
set.seed(1234)  

maxes <- foreach(i = 1:3, .combine = c, .packages = "iterators") %do% {
  it2 <- irnorm(1)  
  max <- foreach(j = 1:5, .combine = c) %do% {
    nextElem(it2)
  }
  max(max)
}

maxes |> 
  t() |> 
  knitr::kable(caption = "Serially computed maximums of Normal random vectors", digits = 3)
```

# Question 5

Below a run time comparison is given for the performance of the parLapply, foreach and replicate functions for the previous problem.

```{r}
#foreach

# Set up parallel cluster
cl <- makeCluster(4)
registerDoParallel(cl)

#Setting seed in parallel safe way
  

# Parallel computation of maximums
tablefy(
  system.time(
    maxes <- foreach(i = 1:3, .combine = c, .packages = c("iterators", "foreach")) %dopar% {
    it2 <- irnorm(1)  
    max_val <- foreach(j = 1:5, .combine = c, .packages = "foreach") %do% {  
      nextElem(it2)
    }
    max(max_val)
  }),
"foreach run time"
)

# Stop cluster
stopCluster(cl)





```

```{r}

#Function f that takes 5 normal draws from an irnorm iterator and finds the max of these draws

f <- function(x){
  library(iterators)
  it <- irnorm(1)
  elements <- sapply(1:5, function(y) nextElem(it))
  max(elements)
  
}

cl <- makeCluster(10)


tablefy(system.time(maxlist <- parLapply(cl, 1:3, f)), "parLapply run time")

stopCluster(cl)

```

```{r}

#Function f that takes 5 normal draws from an irnorm iterator and finds the max of these draws
f <- function(x){
  it <- irnorm(1)
  elements <- sapply(1:5, function(y) nextElem(it))
  max <- max(elements)
}



tablefy(system.time(maxlist <- replicate(3, f(1))), "replicate run time")



```

Clearly the parLapply function is superior as its runtime is so low that it registers as 0.
